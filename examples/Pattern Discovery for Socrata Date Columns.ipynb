{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "north-chemistry",
   "metadata": {},
   "source": [
    "# Identify Patterns for Date Columns from Socrata Datasets\n",
    "\n",
    "Identify patterns for values in a single dominant cluster of terms from columns labeled as **calendar date** in Socrata datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bizarre-hudson",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['EMPLOYMENT_END_DATE.11.g9vh-zeiw.txt.gz',\n",
       " 'DATA_FI_PRORROGA.23.hb6v-jcbf.txt.gz',\n",
       " 'AM_Count.32.w76s-c5u4.txt.gz',\n",
       " 'COISSUEDDATE.26.94uh-66xv.txt.gz']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# All example column files are in the local directory 'resources/data/socrata/dates'.\n",
    "# Files are tab-delimited with two columns: term and frequency count.\n",
    "\n",
    "import os\n",
    "\n",
    "inputdir = '../resources/data/socrata/dates'\n",
    "files = os.listdir(inputdir)\n",
    "\n",
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bridal-force",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The pattern discovery function takes an input file as the only argument.\n",
    "# It first extracts a sample of terms from the file. We then look for a\n",
    "# cluster that covers a large fraction of the terms and derive a pattern\n",
    "# for that cluster as the function result.\n",
    "\n",
    "from openclean.pipeline import stream\n",
    "from openclean_pattern.collect.group import Group\n",
    "from openclean_pattern.regex.compiler import DefaultRegexCompiler\n",
    "from openclean_pattern.tokenize.factory import TokenizerFactory\n",
    "\n",
    "\n",
    "collector = Group()\n",
    "compiler = DefaultRegexCompiler(method='col')\n",
    "tokenizer = TokenizerFactory.create_tokenizer('default')\n",
    "\n",
    "\n",
    "def find_pattern(ds, sample_size=1000, threshold=0.9):\n",
    "    # Get a sample of terms from the column.\n",
    "    terms = list(ds.sample(sample_size, random_state=42).to_df()['term'])\n",
    "    # Tokenize and convert tokens into representation.\n",
    "    tokenized_terms = tokenizer.encode(terms)\n",
    "    # Group tokenized terms by number of tokens.\n",
    "    clusters = collector.collect(tokenized_terms)\n",
    "    for _, term_ids in clusters.items():\n",
    "        if len(term_ids) / len(terms) < threshold:\n",
    "            # Ignore small clusters.\n",
    "            continue\n",
    "        # Return the pattern for the found cluster. This assumes that\n",
    "        # maximally one cluster can satisfy the threshold.\n",
    "        return compiler.compile(tokenized_terms, {0: term_ids})[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dutch-airfare",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing EMPLOYMENT_END_DATE.11.g9vh-zeiw.txt.gz\n",
      "\n",
      "\tPattern: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) 20XX(4-4))\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing DATA_FI_PRORROGA.23.hb6v-jcbf.txt.gz\n",
      "\n",
      "\tPattern: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) 20XX(4-4))\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing AM_Count.32.w76s-c5u4.txt.gz\n",
      "\n",
      "\tPattern: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) NUMERIC(4-4))\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing COISSUEDDATE.26.94uh-66xv.txt.gz\n",
      "\n",
      "\tPattern: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) 20XX(4-4) \\S() 12(2-2) PUNC(:) 00(2-2) PUNC(:) 00(2-2) \\S() am(2-2))\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from openclean.function.eval.base import Eval\n",
    "\n",
    "\n",
    "for filename in files:\n",
    "    print('processing {}'.format(filename))\n",
    "    # Create data stream that returns the column terms only.\n",
    "    ds = stream(os.path.join(inputdir, filename), header=['term', 'freq'], delim='\\t', compressed=True)\\\n",
    "        .select('term')\n",
    "    # Get pattern for largest cluster (if exists).\n",
    "    patterns = find_pattern(ds, sample_size=100)\n",
    "    if patterns:\n",
    "        # The column yielded a pattern. List all terms that\n",
    "        # do not match the pattern.\n",
    "        print('\\n\\tPattern: {}\\n'.format(patterns))\n",
    "        pattern = patterns.top(n=1, pattern=True)\n",
    "        func = pattern.compile(negate=True, tokenizer=tokenizer)\n",
    "        outliers = ds.filter(Eval(columns='term', func=func)).distinct()\n",
    "        if outliers:\n",
    "            for key, _ in outliers.items():\n",
    "                print('\\t{}'.format(key))\n",
    "        else:\n",
    "            print('\\tno outliers')\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('\\n\\tno pattern.\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "local-necklace",
   "metadata": {},
   "source": [
    "## Using the PatternFinder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "lyric-january",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openclean_pattern.opencleanpatternfinder import OpencleanPatternFinder as PF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "mobile-toyota",
   "metadata": {},
   "outputs": [],
   "source": [
    "opf = PF(\n",
    "    frac = 1,\n",
    "    distinct = True,\n",
    "    tokenizer = tokenizer,\n",
    "    collector = collector,\n",
    "    compiler = compiler\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "limiting-culture",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing EMPLOYMENT_END_DATE.11.g9vh-zeiw.txt.gz\n",
      "\n",
      "\tPattern: {5: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) 20XX(4-4))}\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing DATA_FI_PRORROGA.23.hb6v-jcbf.txt.gz\n",
      "\n",
      "\tPattern: {5: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) NUMERIC(4-4))}\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing AM_Count.32.w76s-c5u4.txt.gz\n",
      "\n",
      "\tPattern: {5: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) NUMERIC(4-4))}\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n",
      "processing COISSUEDDATE.26.94uh-66xv.txt.gz\n",
      "\n",
      "\tPattern: {13: RowPatterns(NUMERIC(2-2) PUNC(/) NUMERIC(2-2) PUNC(/) 20XX(4-4) \\S() 12(2-2) PUNC(:) 00(2-2) PUNC(:) 00(2-2) \\S() am(2-2))}\n",
      "\n",
      "\tno outliers\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for filename in files:\n",
    "    print('processing {}'.format(filename))\n",
    "    ds = stream(os.path.join(inputdir, filename), header=['term', 'freq'], delim='\\t', compressed=True)\\\n",
    "        .select('term').to_df()['term']\n",
    "    patterns = opf.find(ds.to_list())\n",
    "    if patterns:\n",
    "        print('\\n\\tPattern: {}\\n'.format(patterns))\n",
    "        outliers = opf.outliers\n",
    "        print('\\toutliers: {}'.format(ds[outliers]).tolist() if not ds[outliers].empty else '\\tno outliers')\n",
    "        print('\\n')\n",
    "    else:\n",
    "        print('\\n\\tno pattern.\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (openclean-core)",
   "language": "python",
   "name": "pycharm-c6d12a8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
